{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We spend a lot of money on education every year! In general, we believe that the more we spend, the better our schools are and the better our students perform. But do we really know that?\n",
    "\n",
    "To adress these questions, we will spend today looking at a US education dataset and see what we can learn about indicators of student performance. In particular, we want to answer the question: what are useful indicators to predict student performance on national exams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Poking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by importing our data and seeing what we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress Pandas SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/states_edu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1715, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given that this dataset describes \"K-12 financial, enrollment, and achievement data in one place\". Each row is one state in one year, and includes variables for revenue categories, expenditure types, enrollment numbers, and exam scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRIMARY_KEY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>ENROLL</th>\n",
       "      <th>TOTAL_REVENUE</th>\n",
       "      <th>FEDERAL_REVENUE</th>\n",
       "      <th>STATE_REVENUE</th>\n",
       "      <th>LOCAL_REVENUE</th>\n",
       "      <th>TOTAL_EXPENDITURE</th>\n",
       "      <th>INSTRUCTION_EXPENDITURE</th>\n",
       "      <th>...</th>\n",
       "      <th>GRADES_4_G</th>\n",
       "      <th>GRADES_8_G</th>\n",
       "      <th>GRADES_12_G</th>\n",
       "      <th>GRADES_1_8_G</th>\n",
       "      <th>GRADES_9_12_G</th>\n",
       "      <th>GRADES_ALL_G</th>\n",
       "      <th>AVG_MATH_4_SCORE</th>\n",
       "      <th>AVG_MATH_8_SCORE</th>\n",
       "      <th>AVG_READING_4_SCORE</th>\n",
       "      <th>AVG_READING_8_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>1998_ALABAMA</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1998</td>\n",
       "      <td>739321.0</td>\n",
       "      <td>4140537.0</td>\n",
       "      <td>374153.0</td>\n",
       "      <td>2589819.0</td>\n",
       "      <td>1176565.0</td>\n",
       "      <td>4245033.0</td>\n",
       "      <td>2218693.0</td>\n",
       "      <td>...</td>\n",
       "      <td>58610.0</td>\n",
       "      <td>57105.0</td>\n",
       "      <td>43957.0</td>\n",
       "      <td>473954.0</td>\n",
       "      <td>205630.0</td>\n",
       "      <td>747980.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>1998_ARIZONA</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>1998</td>\n",
       "      <td>794331.0</td>\n",
       "      <td>4675296.0</td>\n",
       "      <td>470398.0</td>\n",
       "      <td>2000801.0</td>\n",
       "      <td>2204097.0</td>\n",
       "      <td>4726098.0</td>\n",
       "      <td>2091741.0</td>\n",
       "      <td>...</td>\n",
       "      <td>67764.0</td>\n",
       "      <td>63943.0</td>\n",
       "      <td>45813.0</td>\n",
       "      <td>543670.0</td>\n",
       "      <td>224867.0</td>\n",
       "      <td>848262.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1998_ARKANSAS</td>\n",
       "      <td>ARKANSAS</td>\n",
       "      <td>1998</td>\n",
       "      <td>456355.0</td>\n",
       "      <td>2567380.0</td>\n",
       "      <td>226475.0</td>\n",
       "      <td>1505419.0</td>\n",
       "      <td>835486.0</td>\n",
       "      <td>2536027.0</td>\n",
       "      <td>1367612.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35034.0</td>\n",
       "      <td>35936.0</td>\n",
       "      <td>29123.0</td>\n",
       "      <td>282517.0</td>\n",
       "      <td>132507.0</td>\n",
       "      <td>452256.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>209.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>1998_CALIFORNIA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>1998</td>\n",
       "      <td>5727224.0</td>\n",
       "      <td>39183018.0</td>\n",
       "      <td>3149260.0</td>\n",
       "      <td>22840500.0</td>\n",
       "      <td>13193258.0</td>\n",
       "      <td>38087666.0</td>\n",
       "      <td>20083913.0</td>\n",
       "      <td>...</td>\n",
       "      <td>462241.0</td>\n",
       "      <td>424768.0</td>\n",
       "      <td>334852.0</td>\n",
       "      <td>3666271.0</td>\n",
       "      <td>1627284.0</td>\n",
       "      <td>5926037.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202.0</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1998_COLORADO</td>\n",
       "      <td>COLORADO</td>\n",
       "      <td>1998</td>\n",
       "      <td>686360.0</td>\n",
       "      <td>4359021.0</td>\n",
       "      <td>215071.0</td>\n",
       "      <td>1879850.0</td>\n",
       "      <td>2264100.0</td>\n",
       "      <td>4739136.0</td>\n",
       "      <td>2253623.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54382.0</td>\n",
       "      <td>53556.0</td>\n",
       "      <td>40076.0</td>\n",
       "      <td>436825.0</td>\n",
       "      <td>197136.0</td>\n",
       "      <td>699135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>220.0</td>\n",
       "      <td>264.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PRIMARY_KEY       STATE  YEAR     ENROLL  TOTAL_REVENUE  \\\n",
       "306     1998_ALABAMA     ALABAMA  1998   739321.0      4140537.0   \n",
       "308     1998_ARIZONA     ARIZONA  1998   794331.0      4675296.0   \n",
       "309    1998_ARKANSAS    ARKANSAS  1998   456355.0      2567380.0   \n",
       "310  1998_CALIFORNIA  CALIFORNIA  1998  5727224.0     39183018.0   \n",
       "311    1998_COLORADO    COLORADO  1998   686360.0      4359021.0   \n",
       "\n",
       "     FEDERAL_REVENUE  STATE_REVENUE  LOCAL_REVENUE  TOTAL_EXPENDITURE  \\\n",
       "306         374153.0      2589819.0      1176565.0          4245033.0   \n",
       "308         470398.0      2000801.0      2204097.0          4726098.0   \n",
       "309         226475.0      1505419.0       835486.0          2536027.0   \n",
       "310        3149260.0     22840500.0     13193258.0         38087666.0   \n",
       "311         215071.0      1879850.0      2264100.0          4739136.0   \n",
       "\n",
       "     INSTRUCTION_EXPENDITURE  ...  GRADES_4_G  GRADES_8_G  GRADES_12_G  \\\n",
       "306                2218693.0  ...     58610.0     57105.0      43957.0   \n",
       "308                2091741.0  ...     67764.0     63943.0      45813.0   \n",
       "309                1367612.0  ...     35034.0     35936.0      29123.0   \n",
       "310               20083913.0  ...    462241.0    424768.0     334852.0   \n",
       "311                2253623.0  ...     54382.0     53556.0      40076.0   \n",
       "\n",
       "     GRADES_1_8_G  GRADES_9_12_G  GRADES_ALL_G  AVG_MATH_4_SCORE  \\\n",
       "306      473954.0       205630.0      747980.0               NaN   \n",
       "308      543670.0       224867.0      848262.0               NaN   \n",
       "309      282517.0       132507.0      452256.0               NaN   \n",
       "310     3666271.0      1627284.0     5926037.0               NaN   \n",
       "311      436825.0       197136.0      699135.0               NaN   \n",
       "\n",
       "     AVG_MATH_8_SCORE  AVG_READING_4_SCORE  AVG_READING_8_SCORE  \n",
       "306               NaN                211.0                255.0  \n",
       "308               NaN                206.0                260.0  \n",
       "309               NaN                209.0                256.0  \n",
       "310               NaN                202.0                252.0  \n",
       "311               NaN                220.0                264.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PRIMARY_KEY', 'STATE', 'YEAR', 'ENROLL', 'TOTAL_REVENUE',\n",
       "       'FEDERAL_REVENUE', 'STATE_REVENUE', 'LOCAL_REVENUE',\n",
       "       'TOTAL_EXPENDITURE', 'INSTRUCTION_EXPENDITURE',\n",
       "       'SUPPORT_SERVICES_EXPENDITURE', 'OTHER_EXPENDITURE',\n",
       "       'CAPITAL_OUTLAY_EXPENDITURE', 'GRADES_PK_G', 'GRADES_KG_G',\n",
       "       'GRADES_4_G', 'GRADES_8_G', 'GRADES_12_G', 'GRADES_1_8_G',\n",
       "       'GRADES_9_12_G', 'GRADES_ALL_G', 'AVG_MATH_4_SCORE', 'AVG_MATH_8_SCORE',\n",
       "       'AVG_READING_4_SCORE', 'AVG_READING_8_SCORE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename our columns to make them more intuitive\n",
    "df.rename({\n",
    "    'GRADES_PK_G':'ENROLL_PREK',\n",
    "    'GRADES_KG_G':'ENROLL_KINDER',\n",
    "    'GRADES_4_G':'ENROLL_4',\n",
    "    'GRADES_8_G':'ENROLL_8',\n",
    "    'GRADES_12_G':'ENROLL_12',\n",
    "    'GRADES_1_8_G':'ENROLL_PRIMARY',\n",
    "    'GRADES_9_12_G':'ENROLL_HS',\n",
    "    'GRADES_ALL_G':'ENROLL_ALL',\n",
    "    'ENROLL':'ENROLL_ALL_EST'\n",
    "    },\n",
    "    axis=1,inplace=True)\n",
    "#inplace return copy of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRIMARY_KEY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>ENROLL</th>\n",
       "      <th>TOTAL_REVENUE</th>\n",
       "      <th>FEDERAL_REVENUE</th>\n",
       "      <th>STATE_REVENUE</th>\n",
       "      <th>LOCAL_REVENUE</th>\n",
       "      <th>TOTAL_EXPENDITURE</th>\n",
       "      <th>INSTRUCTION_EXPENDITURE</th>\n",
       "      <th>...</th>\n",
       "      <th>GRADES_4_G</th>\n",
       "      <th>GRADES_8_G</th>\n",
       "      <th>GRADES_12_G</th>\n",
       "      <th>GRADES_1_8_G</th>\n",
       "      <th>GRADES_9_12_G</th>\n",
       "      <th>GRADES_ALL_G</th>\n",
       "      <th>AVG_MATH_4_SCORE</th>\n",
       "      <th>AVG_MATH_8_SCORE</th>\n",
       "      <th>AVG_READING_4_SCORE</th>\n",
       "      <th>AVG_READING_8_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>1998_ALABAMA</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1998</td>\n",
       "      <td>739321.0</td>\n",
       "      <td>4140537.0</td>\n",
       "      <td>374153.0</td>\n",
       "      <td>2589819.0</td>\n",
       "      <td>1176565.0</td>\n",
       "      <td>4245033.0</td>\n",
       "      <td>2218693.0</td>\n",
       "      <td>...</td>\n",
       "      <td>58610.0</td>\n",
       "      <td>57105.0</td>\n",
       "      <td>43957.0</td>\n",
       "      <td>473954.0</td>\n",
       "      <td>205630.0</td>\n",
       "      <td>747980.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>1998_ARIZONA</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>1998</td>\n",
       "      <td>794331.0</td>\n",
       "      <td>4675296.0</td>\n",
       "      <td>470398.0</td>\n",
       "      <td>2000801.0</td>\n",
       "      <td>2204097.0</td>\n",
       "      <td>4726098.0</td>\n",
       "      <td>2091741.0</td>\n",
       "      <td>...</td>\n",
       "      <td>67764.0</td>\n",
       "      <td>63943.0</td>\n",
       "      <td>45813.0</td>\n",
       "      <td>543670.0</td>\n",
       "      <td>224867.0</td>\n",
       "      <td>848262.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1998_ARKANSAS</td>\n",
       "      <td>ARKANSAS</td>\n",
       "      <td>1998</td>\n",
       "      <td>456355.0</td>\n",
       "      <td>2567380.0</td>\n",
       "      <td>226475.0</td>\n",
       "      <td>1505419.0</td>\n",
       "      <td>835486.0</td>\n",
       "      <td>2536027.0</td>\n",
       "      <td>1367612.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35034.0</td>\n",
       "      <td>35936.0</td>\n",
       "      <td>29123.0</td>\n",
       "      <td>282517.0</td>\n",
       "      <td>132507.0</td>\n",
       "      <td>452256.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>209.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>1998_CALIFORNIA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>1998</td>\n",
       "      <td>5727224.0</td>\n",
       "      <td>39183018.0</td>\n",
       "      <td>3149260.0</td>\n",
       "      <td>22840500.0</td>\n",
       "      <td>13193258.0</td>\n",
       "      <td>38087666.0</td>\n",
       "      <td>20083913.0</td>\n",
       "      <td>...</td>\n",
       "      <td>462241.0</td>\n",
       "      <td>424768.0</td>\n",
       "      <td>334852.0</td>\n",
       "      <td>3666271.0</td>\n",
       "      <td>1627284.0</td>\n",
       "      <td>5926037.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202.0</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1998_COLORADO</td>\n",
       "      <td>COLORADO</td>\n",
       "      <td>1998</td>\n",
       "      <td>686360.0</td>\n",
       "      <td>4359021.0</td>\n",
       "      <td>215071.0</td>\n",
       "      <td>1879850.0</td>\n",
       "      <td>2264100.0</td>\n",
       "      <td>4739136.0</td>\n",
       "      <td>2253623.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54382.0</td>\n",
       "      <td>53556.0</td>\n",
       "      <td>40076.0</td>\n",
       "      <td>436825.0</td>\n",
       "      <td>197136.0</td>\n",
       "      <td>699135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>220.0</td>\n",
       "      <td>264.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PRIMARY_KEY       STATE  YEAR     ENROLL  TOTAL_REVENUE  \\\n",
       "306     1998_ALABAMA     ALABAMA  1998   739321.0      4140537.0   \n",
       "308     1998_ARIZONA     ARIZONA  1998   794331.0      4675296.0   \n",
       "309    1998_ARKANSAS    ARKANSAS  1998   456355.0      2567380.0   \n",
       "310  1998_CALIFORNIA  CALIFORNIA  1998  5727224.0     39183018.0   \n",
       "311    1998_COLORADO    COLORADO  1998   686360.0      4359021.0   \n",
       "\n",
       "     FEDERAL_REVENUE  STATE_REVENUE  LOCAL_REVENUE  TOTAL_EXPENDITURE  \\\n",
       "306         374153.0      2589819.0      1176565.0          4245033.0   \n",
       "308         470398.0      2000801.0      2204097.0          4726098.0   \n",
       "309         226475.0      1505419.0       835486.0          2536027.0   \n",
       "310        3149260.0     22840500.0     13193258.0         38087666.0   \n",
       "311         215071.0      1879850.0      2264100.0          4739136.0   \n",
       "\n",
       "     INSTRUCTION_EXPENDITURE  ...  GRADES_4_G  GRADES_8_G  GRADES_12_G  \\\n",
       "306                2218693.0  ...     58610.0     57105.0      43957.0   \n",
       "308                2091741.0  ...     67764.0     63943.0      45813.0   \n",
       "309                1367612.0  ...     35034.0     35936.0      29123.0   \n",
       "310               20083913.0  ...    462241.0    424768.0     334852.0   \n",
       "311                2253623.0  ...     54382.0     53556.0      40076.0   \n",
       "\n",
       "     GRADES_1_8_G  GRADES_9_12_G  GRADES_ALL_G  AVG_MATH_4_SCORE  \\\n",
       "306      473954.0       205630.0      747980.0               NaN   \n",
       "308      543670.0       224867.0      848262.0               NaN   \n",
       "309      282517.0       132507.0      452256.0               NaN   \n",
       "310     3666271.0      1627284.0     5926037.0               NaN   \n",
       "311      436825.0       197136.0      699135.0               NaN   \n",
       "\n",
       "     AVG_MATH_8_SCORE  AVG_READING_4_SCORE  AVG_READING_8_SCORE  \n",
       "306               NaN                211.0                255.0  \n",
       "308               NaN                206.0                260.0  \n",
       "309               NaN                209.0                256.0  \n",
       "310               NaN                202.0                252.0  \n",
       "311               NaN                220.0                264.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking closer at the data, there are a lot of 'NaN' values... what are those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a numpy value which represents missnig or invalid data (not-a-number)\n",
    "np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is treated as a float, so it is easily compatible with numpy and pandas\n",
    "type(np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily find and describe missing values with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PRIMARY_KEY                       0\n",
       "STATE                             0\n",
       "YEAR                              0\n",
       "ENROLL                          126\n",
       "TOTAL_REVENUE                   126\n",
       "FEDERAL_REVENUE                 126\n",
       "STATE_REVENUE                   126\n",
       "LOCAL_REVENUE                   126\n",
       "TOTAL_EXPENDITURE               126\n",
       "INSTRUCTION_EXPENDITURE         126\n",
       "SUPPORT_SERVICES_EXPENDITURE    126\n",
       "OTHER_EXPENDITURE               126\n",
       "CAPITAL_OUTLAY_EXPENDITURE      126\n",
       "GRADES_PK_G                      78\n",
       "GRADES_KG_G                      75\n",
       "GRADES_4_G                       75\n",
       "GRADES_8_G                       75\n",
       "GRADES_12_G                      75\n",
       "GRADES_1_8_G                    126\n",
       "GRADES_9_12_G                    75\n",
       "GRADES_ALL_G                     75\n",
       "AVG_MATH_4_SCORE                 84\n",
       "AVG_MATH_8_SCORE                 84\n",
       "AVG_READING_4_SCORE               0\n",
       "AVG_READING_8_SCORE               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will print the number of missing values in each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will print the number of valid values in each column\n",
    "df.notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that pandas will often ignore missing values by default\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can deal with missing values is by dropping rows with any null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, dropna will remove all rows with at least 1 nan\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping rows with any nan leaves us only 355 rows -- do we actually need all our data to be complete? Which rows are actually important?\n",
    "\n",
    "That depends on what you want to do with the data! \n",
    "\n",
    "For the purpose of this tutorial, let's say we are particularly interested in 8th grade reading scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In that case, we will drop all the rows where the 8th grading reading score is missing\n",
    "df.dropna(subset=['AVG_READING_8_SCORE'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of dealing with missing values is filling them in with a value that is representative of other values in the column. Medians and means are common choices and are suited to different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data, we have two columns representing total student enrollment: `ENROLL_ALL_EST` and `ENROLL_ALL`. We also have enrollment data divided by school group. Let's see if we can use them to fill each other in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"ENROLL_ALL\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's check if the individual enrollments actually sum up to total enrollment\n",
    "(df[\"ENROLL_ALL\"]-df[\"ENROLL_PREK\"]-df[\"ENROLL_KINDER\"]-df[\"ENROLL_PRIMARY\"]-df[\"ENROLL_HS\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrollment differences as a percent\n",
    "((df[\"ENROLL_ALL\"]-df[\"ENROLL_PREK\"]-df[\"ENROLL_KINDER\"]-df[\"ENROLL_PRIMARY\"]-df[\"ENROLL_HS\"])/df[\"ENROLL_ALL\"]*100).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the individual enrollments do sum up to the total enrollment in most cases! And even when they don't, the deviation is usually not drastic.\n",
    "\n",
    "This is not a terrible way to estimate total enrollment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ENROLL_ALL'] = df['ENROLL_ALL'].fillna(df[\"ENROLL_PREK\"]+df[\"ENROLL_PRIMARY\"]+df[\"ENROLL_HS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this didn't actually do anything!\n",
    "df[\"ENROLL_ALL\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns out, data missing ENROLL_ALL is also missing all other enrollment data\n",
    "df[df[\"ENROLL_ALL\"].isna()][['ENROLL_PREK','ENROLL_PRIMARY','ENROLL_HS','ENROLL_ALL_EST']].notna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but there are rows with enrollment estimates\n",
    "df[df.ENROLL_ALL_EST.isna()][\"ENROLL_ALL\"].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if we can fill these in\n",
    "((df[\"ENROLL_ALL\"] - df[\"ENROLL_ALL_EST\"])/df[\"ENROLL_ALL\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the average error between estimated and actual enrollment is ~2%, I'm going to go ahead and fill in the missing estimates\n",
    "df[\"ENROLL_ALL_EST\"] = df[\"ENROLL_ALL_EST\"].fillna(df[\"ENROLL_ALL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we just did was data cleanup! Most data scientists will tell you that data cleanup and preprocessing will take >60% of the total time for a given project... We just gave you a small teaser here but you'll be seeing a lot more of it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something else you'll see a lot of is feature engineering. In this step, we manipulate the data set so the data is can be used for analysis more readily.\n",
    "\n",
    "Here are some common methods of modifying features:\n",
    "\n",
    "* Standardization\n",
    ">helps some models account for different magnitude features, e.g. revenue is ~10x bigger than enrollment on average, but that doesn't make it more important\n",
    "* Binning\n",
    ">reduces the importance of small differences in data, e.g. exact enrollment probably doesn't matter, but there may still be a difference between 'small', 'medium', and 'large' schools\n",
    "* Combining features\n",
    ">combinations of features may matter more than the features on their own, e.g. educational expenditure as a percent of total expenditure is more informative about a state's priorities (states aren't all the same size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this case, we know our data is on the state level and also longitudinal (over time). \n",
    "\n",
    "This format introduces some complications. For example, the state of California will obviously spend more than New Jersey becuase they have more people... how can we account for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a new column which represents expenditure per student\n",
    "df['SUPPORT_SERVICES_EXPENDITURE_PER_STUDENT'] = df['SUPPORT_SERVICES_EXPENDITURE'] / df['ENROLL_ALL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some EDA (exploratory data analysis)!\n",
    "\n",
    "You should always perform EDA when you are beginning to work with a new dataset. EDA will reveal irregularities and interesting patterns in the data, both of which are hugely informative for your work later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in EDA is usually looking at the variable of interest in isolation. What's its distribution? How has it changed over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note - this test is scored out of 500 according to the NAEP website\n",
    "df.AVG_READING_8_SCORE.plot.hist(title=\"Distribution of 8th Grade Reading Scores\", edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('YEAR')[\"AVG_READING_8_SCORE\"].mean().plot()\n",
    "plt.ylabel('SCORE')\n",
    "plt.title('8th Grade Reading Score Over Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can investigate the relationship between the variable of interest and other (potentially) relevant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='ENROLL_8', y='AVG_READING_8_SCORE', alpha=0.6)\n",
    "plt.xlabel('8th Grade Enrollment')\n",
    "plt.ylabel('8th Grade Reading Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='STATE_REVENUE', y='AVG_READING_8_SCORE', alpha=0.6)\n",
    "plt.xlabel('State Revenue')\n",
    "plt.ylabel('8th Grade Reading Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='INSTRUCTION_EXPENDITURE', y='AVG_READING_8_SCORE', alpha=0.6)\n",
    "plt.xlabel('Instruction Expenditure')\n",
    "plt.ylabel('8th Grade Reading Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='AVG_READING_4_SCORE', y='AVG_READING_8_SCORE', alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='AVG_MATH_8_SCORE', y='AVG_READING_8_SCORE', alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems 4th grade reading score and 8th grade math score are strongly correlated with 8th grade reading score. All the other variables that we investigated have weak or no correlation with 8th grade reading score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we know a bit about the data, what do we want to do with it? How am I going to frame this as a _machine learning_ project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Intro to Machine Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we can't teach machine learning in single tutorial. For this tutorial, we're going to practice a simple _supervised learning_ problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine learning workflow:**\n",
    "<img src=https://miro.medium.com/proxy/1*KzmIUYPmxgEHhXX7SlbP4w.jpeg width=500></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised learning:**\n",
    "<img src=https://miro.medium.com/max/1050/1*-fniNC8gWI34qLAiBzgGZA.png width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have established that we are interested in 8th grade reading scores, so I want to make that my response variable (i.e. what I'm trying to predict).\n",
    "\n",
    "Based on the EDA, I think that `ENROLL_8`, `AVG_MATH_8_SCORE`, and `AVG_READING_4_SCORE` would be interesting predictors to look at, so I will pick these as my input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_train_split randomly splits the data into two parts -- \n",
    "# one for training the model (it uses this data to learn patterns)\n",
    "# and one for testing the model (to make sure it performs well on data it hasn't seen before)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is commonly used to denote the input data\n",
    "# y is used for the response / output data\n",
    "X = df[['ENROLL_8','AVG_MATH_8_SCORE','AVG_READING_4_SCORE']].dropna()\n",
    "y = df.loc[X.index]['AVG_READING_8_SCORE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to make sure there is no NaN in y\n",
    "# This time, we will fill the NaN with the median of y \n",
    "# We prefer median to mean because EDA reveals that the response variable is left-skewed. Therefore, the mean may not represent the data very well\n",
    "y.fillna(y.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the test_size parameter defines what % of data is set aside for testing, 70 / 30 and 80 / 20 split are both typical\n",
    "# we don't have a huge data set but we still want to have a decently sized testing set\n",
    "# so we are using a 70 / 30 train / test split. \n",
    "# setting random_state explicitly ensures that I get the same results each time I run the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to create and train a model! For simplicity, I'm going to use `sklearn`'s `LinearRegression` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit is essentially the word sklearn uses for training\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are doing here is called _least squares linear regression_. \n",
    "\n",
    "Let's say there are $k$ input variables, named $x_1$ through $x_k$ (here, I have $k=3$, $x_1$ = `ENROLL_8`, $x_2$ = `AVG_MATH_8_SCORE`, etc.)\n",
    "\n",
    "The model is trying to find the one equation of the form that minimizes some error measure. In this case, that measure is residual sum of squares ([RSS](https://en.wikipedia.org/wiki/Residual_sum_of_squares)):\n",
    "\n",
    "$y_{predicted} = intercept + \\beta_0x_1 + \\beta_1x_2 + ... + \\beta_kx_k$ where $\\beta_i$ are the coefficients. \n",
    "\n",
    "Notice there are exactly $k$ coefficients. We can interpret each coefficient by holding all other variables constant (_ceteris paribus_, if you are feeling fancy). \n",
    "\n",
    "For example, if $\\beta_2=0.2$, we say \"with all other variables held constant, a 1 point increase in average grade 8 math score results in a 0.2-point increase in reading score\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see the intercepts and coefficients the model generates\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 value describes how well a linear model fits the data\n",
    "# It ranges between 0 and 1\n",
    "# There are many caveats to R^2 but it is a good starting point\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean error\n",
    "np.mean(model.predict(X_test)-y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean absolute error\n",
    "np.mean(np.abs(model.predict(X_test)-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error -- penalizes large errors\n",
    "np.mean((model.predict(X_test)-y_test)**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the model's predictions and how it differs from the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = 'AVG_MATH_8_SCORE'\n",
    "\n",
    "f = plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_train[col_name], y_train, color = \"red\")\n",
    "plt.scatter(X_train[col_name], model.predict(X_train), color = \"green\")\n",
    "\n",
    "plt.legend(['True Training','Predicted Training'])\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('Reading 8 score')\n",
    "plt.title(\"Model Behavior On Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = 'AVG_MATH_8_SCORE'\n",
    "\n",
    "f = plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_test[col_name], y_test, color = \"blue\")\n",
    "plt.scatter(X_test[col_name], model.predict(X_test), color = \"black\")\n",
    "\n",
    "plt.legend(['True testing','Predicted testing'])\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('Reading 8 score')\n",
    "plt.title(\"Model Behavior on Testing Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem that our model works fairly well on the training set and also generalizes nicely to the testing set. This is a good thing! Sometimes models will work *too* well on the training set that it does poorly on the testing set. \n",
    "\n",
    "This is known as overfitting. We will have a lot more to say about it in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e78b6b4158d8f577a77be3bef6c4f5889b406541923fa59adc2e6c48950512fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
